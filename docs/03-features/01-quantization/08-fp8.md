---
title: FP8 W8A8
---

[\*在线运行 vLLM 入门教程：零基础分步指南](https://openbayes.com/console/public/tutorials/rXxb5fZFr29?utm_source=vLLM-CNdoc&utm_medium=vLLM-CNdoc-V1&utm_campaign=vLLM-CNdoc-V1-25ap)

vLLM 使用 Nvidia H100 和 AMD MI300x 等 GPU 上的硬件加速时，支持 FP8（8 位浮点）权重和激活量化。目前，W8A8 仅正式支持 Hopper 和 Ada Lovelace GPU。使用 Marlin 内核的 W8A16（仅权重 FP8）支持 Ampere GPU。使用 FP8 进行模型量化可将模型内存需求减少 2 倍，并将吞吐量提高 1.6 倍，同时对准确性的影响最小。

请访问 HF 平台上[可与 vLLM 一起使用的流行 LLM 的量化 FP8 检查点的集合](https://huggingface.co/collections/neuralmagic/fp8-llms-for-vllm-666742ed2b78b7ac8df13127)。

硬件中通常支持的 FP8 类型有两种不同的表示形式，每种表示形式在不同的场景中都有用：

- **E4M3**：由 1 位符号位、4 位指数位和 3 位尾数组成。它可以存储高达 +/-448 和 `nan` 的值。
- **E5M2**：由 1 位符号位、5 位指数位和 2 位尾数组成。它可以存储高达 +/-57344、+/- `inf` 和 `nan` 的值。增加动态范围的代价是存储值的精度较低。

> **注意：**
> FP8 计算支持计算能力 > 8.9 的 NVIDIA GPU (Ada Lovelace，Hopper)。FP8 模型将在计算能力 > 8.0 (Ampere) 的 GPU 上以仅权重 W8A16 运行，使用 FP8 Marlin。

## 在线动态量化快速入门

使用 vLLM 可以实现原始精密为 BF16/FP16 的模型动态量化为 FP8，而无需任何校准数据。您可以通过在命令行中指定 `--quantization="fp8"` 或在 LLM 构造函数中设置 `quantization="fp8"` 来启用该功能。

在此模式下，所有 Linear 模块（最终的 `lm_head` 除外）的权重均按张量比例量化至 FP8_E4M3 精度。激活值在每次前向传播期间计算其最小值和最大值，以提供动态的每个张量比例因子，从而实现高精度。因此在此模式下，改进延迟会受到限制。

```python
from vllm import LLM
model = LLM("facebook/opt-125m", quantization="fp8")
# INFO 06-10 17:55:42 model_runner.py:157] 加载模型权重占用了 0.1550 GB


result = model.generate("Hello, my name is")
```

**警告**

目前，我们在将模型量化到8位之前会以原始精度加载它，因此您需要足够的内存来加载整个模型。

## 安装

要使用 vLLM 生成高性能 FP8 量化模型，您需要安装 [llm-compressor](https://github.com/vllm-project/llm-compressor/) 库：

```plain
pip install llmcompressor==0.1.0
```

## 量化过程

量化过程涉及 3 个主要步骤：

1. 加载模型

2. 应用量化

3. 评估 vLLM 的准确性

### 1. 加载模型

使用标准 transformers AutoModel 类加载您的模型和分词器：

```python
from llmcompressor.transformers import SparseAutoModelForCausalLM
from transformers import AutoTokenizer


MODEL_ID = "meta-llama/Meta-Llama-3-8B-Instruct"


model = SparseAutoModelForCausalLM.from_pretrained(
  MODEL_ID, device_map="auto", torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
```

### 2. 应用量化

对于 FP8 量化，我们可以通过简单的 RTN 量化来恢复精度。我们建议使用 `FP8_DYNAMIC` 方案定位所有 `Linear` 层，该方案使用：

- 静态的权重每通道量化
- 动态的激活每 token 量化

由于简单的 RTN 不需要权重量化数据，并且激活是动态量化的，因此我们不需要任何校准数据来进行这个量化流程。

```python
from llmcompressor.transformers import oneshot
from llmcompressor.modifiers.quantization import QuantizationModifier


# 配置简单 PTQ 量化


recipe = QuantizationModifier(
  targets="Linear", scheme="FP8_DYNAMIC", ignore=["lm_head"])


# 应用量化算法。


oneshot(model=model, recipe=recipe)


# 保存模型。


SAVE_DIR = MODEL_ID.split("/")[1] + "-FP8-Dynamic"
model.save_pretrained(SAVE_DIR)
tokenizer.save_pretrained(SAVE_DIR)
```

### 3. 评估准确性

安装 `vllm` 和 `lm-evaluation-harness`：

```plain
pip install vllm lm_eval==0.4.3
```

在 `vllm` 中加载并运行模型：

```python
from vllm import LLM
model = LLM("./Meta-Llama-3-8B-Instruct-FP8-Dynamic")
model.generate("Hello my name is")
```

使用 `lm_eval` 评估准确性（例如，在 `gsm8k` 的 250 个样本上）：

**注意：**

量化模型可能对 `bos` token 的存在敏感。默认情况下，`lm_eval` 不会添加 `bos` token，因此请确保在运行评估时包含 `add_bos_token=True` 参数。

```plain
MODEL=$PWD/Meta-Llama-3-8B-Instruct-FP8-Dynamic
lm_eval \
  --model vllm \
  --model_args pretrained=$MODEL,add_bos_token=True \
  --tasks gsm8k  --num_fewshot 5 --batch_size auto --limit 250
```

以下是所得分数的示例：

```plain
|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|
|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|
|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.768|±  |0.0268|
|     |       |strict-match    |     5|exact_match|↑  |0.768|±  |0.0268|
```

## 故障排除和支持

如果您遇到任何问题或有功能请求，请在 `vllm-project/llm-compressor` GitHub 存储库上提出问题。

## 已弃用的流程

> **注意：**
> 保留以下信息以供参考和搜索。下面描述的量化方法已被弃用，取而代之的是上面描述的 `llmcompressor` 方法。

对于 FP8 的静态每张量离线量化，请安装 [AutoFP8 库](https://github.com/neuralmagic/autofp8)。

```bash
git clone https://github.com/neuralmagic/AutoFP8.git
pip install -e AutoFP8
```

该包引入了 `AutoFP8ForCausalLM` 和 `BaseQuantizeConfig` 对象，用于管理模型的压缩方式。

## 使用静态激活缩放因子进行离线量化

您可以使用带有校准数据的 AutoFP8，通过启用 `activation_scheme="static"` 参数为权重和激活生成每个张量的静态缩放。

```python
from datasets import load_dataset
from transformers import AutoTokenizer
from auto_fp8 import AutoFP8ForCausalLM, BaseQuantizeConfig


pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"
quantized_model_dir = "Meta-Llama-3-8B-Instruct-FP8"


tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token


# 加载并标记 512 个数据集样本以校准激活量表


ds = load_dataset("mgoin/ultrachat_2k", split="train_sft").select(range(512))
examples = [tokenizer.apply_chat_template(batch["messages"], tokenize=False) for batch in ds]
examples = tokenizer(examples, padding=True, truncation=True, return_tensors="pt").to("cuda")


# 使用静态激活尺度定义量化配置


quantize_config = BaseQuantizeConfig(quant_method="fp8", activation_scheme="static")


# 加载模型、量化并保存 checkpoint


model = AutoFP8ForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)
model.quantize(examples)
model.save_quantized(quantized_model_dir)
```

具有量化权重和激活的模型 checkpoint 应可在 `Meta-Llama-3-8B-Instruct-FP8/` 处查找。最后，您可以直接在 vLLM 中加载量化模型的 checkpoint。

```python
from vllm import LLM
model = LLM(model="Meta-Llama-3-8B-Instruct-FP8/")
# INFO 06-10 21:15:41 model_runner.py:159] 加载模型权重占用了 8.4596 GB


result = model.generate("Hello, my name is")
```
