---
title: 离线推理
---

[\*在线运行 vLLM 入门教程：零基础分步指南](https://openbayes.com/console/public/tutorials/rXxb5fZFr29?utm_source=vLLM-CNdoc&utm_medium=vLLM-CNdoc-V1&utm_campaign=vLLM-CNdoc-V1-25ap)

离线推理示例展示了如何在离线环境中使用 vLLM，以批量方式查询模型进行预测。我们建议从 [Basic](https://docs.vllm.ai/en/latest/getting_started/examples/basic.html) 开始。

示例

- [Audio Language](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/audio_language)
- [Basic](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/basic)
- [Chat With Tools](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/chat_with_tools)
- [CPU Offload Lmcache](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/cpu_offload_lmcache)
- [Data Parallel](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/data_parallel)
- [分解预填充](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/disaggregated_prefill)
- [Lmcache 分解预填充](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/disaggregated_prefill_lmcache)
- [Distributed](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/distributed)
- [Eagle](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/eagle)
- [Encoder Decoder](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/encoder_decoder)
- [Encoder Decoder Multimodal](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/encoder_decoder_multimodal)
- [LLM Engine Example](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/llm_engine_example)
- [Load Sharded State](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/load_sharded_state)
- [LoRA With Quantization Inference](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/lora_with_quantization_inference)
- [Mistral-Small](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/mistral-small)
- [MLPSpeculator](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/mlpspeculator)
- [MultiLoRA Inference](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/multilora_inference)
- [Neuron](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/neuron)
- [Neuron INT8 Quantization](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/neuron_int8_quantization)
- [使用 OpenAI 批处理文件格式进行离线推理](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/openai_batch)
- [Prefix Caching](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/prefix_caching)
- [Prithvi Geospatial Mae](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/prithvi_geospatial_mae)
- [Profiling](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/profiling)
- [vLLM TPU Profiling](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/profiling_tpu)
- [Reproduciblity](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/reproduciblity)
- [RLHF](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/rlhf)
- [RLHF Colocate](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/rlhf_colocate)
- [RLHF Utils](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/rlhf_utils)
- [Save Sharded State](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/save_sharded_state)
- [Simple Profiling](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/simple_profiling)
- [Structured Outputs](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/structured_outputs)
- [Torchrun Example](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/torchrun_example)
- [TPU](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/tpu)
- [Vision Language](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/vision_language)
- [Vision Language Embedding](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/vision_language_embedding)
- [Vision Language Multi Image](https://vllm.hyper.ai/docs/getting-started/examples/offline-inference/vision_language_multi_image)
