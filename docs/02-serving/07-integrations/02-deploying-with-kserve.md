---
title: 使用 KServe 进行部署
---


vLLM 可以通过 [KServe](https://github.com/kserve/kserve) 在 Kubernetes 上进行部署，以实现高度可扩展的分布式模型服务。


请参阅[该指南](https://kserve.github.io/website/latest/modelserving/v1beta1/llm/vllm/)，了解有关将 vLLM 与 KServe 结合使用的更多详细信息。

