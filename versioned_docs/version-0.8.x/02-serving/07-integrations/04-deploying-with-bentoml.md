---
title: 使用 BentoML 进行部署
---

[BentoML](https://github.com/bentoml/BentoML) 允许您以 vLLM 作为后端部署大型语言模型 (LLM) 服务器，该服务器提供与OpenAI兼容的端点。您可以在本地提供模型，也可以将其容器化为符合 OCI 的镜像，并将其部署在 Kubernetes 上。

有关详细信息，请参阅 [BentoML 文档中的 vLLM 推理](https://docs.bentoml.com/en/latest/use-cases/large-language-models/vllm.html) 教程。
